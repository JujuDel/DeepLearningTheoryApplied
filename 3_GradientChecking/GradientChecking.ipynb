{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking\n",
    "\n",
    "On this notebook, we'll briefly implement gradient checking to ensure that our gradient computations are correct. To do so, we'll use the first derivative formula:\n",
    "\n",
    "$$f'(h) = lim_{\\epsilon->0} \\frac{f(h + \\epsilon) - f(h - \\epsilon)}{2 \\, \\epsilon} \\tag{1}$$\n",
    "\n",
    "With $\\epsilon \\simeq 10^{-7}$:\n",
    "\n",
    "$$f_{approx}'(h) = \\frac{f(h + \\epsilon) - f(h - \\epsilon)}{2 \\, \\epsilon} \\tag{2}$$\n",
    "\n",
    "\n",
    "Then, with\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(A^{[L](i)}, Y^{(i)})\\tag{3}$$\n",
    "\n",
    "We can write:\n",
    "$$ \\frac{\\partial J}{\\partial h}_{approx} = \\frac{J(h + \\epsilon) - J(h - \\epsilon)}{2 \\, \\epsilon}\\tag{4}$$\n",
    "\n",
    "## What to do?\n",
    "\n",
    "At the end of this notebook, we'll start again the cat example using the gradient check technique on the first gradients.\n",
    "\n",
    "## Organization of this Notebook \n",
    "    1. Packages\n",
    "    2. Gradient checking\n",
    "    3. Model construction with Gradient checking\n",
    "    4. Try with the cat training set and the 2_L-LayerNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "- [numpy](www.numpy.org): the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org): a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org): a famous library to plot graphs in Python.\n",
    "- [L_NN](L_NN.py): my own L-layer Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from L_NN import initialize, globalForwardPropagation, computeCost, globalBackwardPropagation, updateParameters\n",
    "from py_utils import load_dataset # Copy/pasted loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Gradient checking ##\n",
    "\n",
    "As we are using dictionnaries, to vectorize the code we need to implement np.array TO Dictionnaries functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParamNpArrayToDict(params, layers_dims, actFunctions):\n",
    "    parameters = {}\n",
    "    \n",
    "    L = len(layerDims)\n",
    "    ptr = 0\n",
    "    for l in range(1, L):\n",
    "        next_ptr = ptr + layers_dims[l]*layers_dims[l-1]\n",
    "        parameters['W' + str(l)] = params[ptr:next_ptr].reshape((layers_dims[l],layers_dims[l-1]))\n",
    "        ptr = next_ptr\n",
    "        \n",
    "        next_ptr = ptr + layers_dims[l]\n",
    "        parameters['b' + str(l)] = params[ptr:next_ptr].reshape((layers_dims[l],1))\n",
    "        ptr = next_ptr\n",
    "        \n",
    "        parameters['F' + str(l)] = actFunctions[l-1]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def ParamDictToNpArray(parameters):\n",
    "    L = len(parameters) // 3\n",
    "    \n",
    "    new_param = np.reshape(parameters[\"W1\"], (-1,1))\n",
    "    params = new_param\n",
    "    \n",
    "    new_param = np.reshape(parameters[\"b1\"], (-1,1))\n",
    "    params = np.concatenate((params, new_param), axis=0)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        new_paramW = np.reshape(parameters['W' + str(l+1)], (-1,1))\n",
    "        new_paramb = np.reshape(parameters['b' + str(l+1)], (-1,1))\n",
    "        \n",
    "        params = np.concatenate((params, new_paramW), axis=0)\n",
    "        params = np.concatenate((params, new_paramb), axis=0)\n",
    "    \n",
    "    return params\n",
    "\n",
    "def GrdtDictToNpArray(gradients):\n",
    "    L = len(gradients) // 3\n",
    "    \n",
    "    new_gradient = np.reshape(gradients[\"dW1\"], (-1,1))\n",
    "    grads = new_gradient\n",
    "    \n",
    "    new_gradient = np.reshape(gradients[\"db1\"], (-1,1))\n",
    "    grads = np.concatenate((grads, new_gradient), axis=0)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        new_gradientW = np.reshape(gradients[\"dW\" + str(l+1)], (-1,1))\n",
    "        new_gradientb = np.reshape(gradients[\"db\" + str(l+1)], (-1,1))\n",
    "        \n",
    "        grads = np.concatenate((grads, new_gradientW), axis=0)\n",
    "        grads = np.concatenate((grads, new_gradientb), axis=0)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the gradient checking. We will use the following:\n",
    "\n",
    "$$ J^+ = J(param+\\epsilon)\\tag{5}$$\n",
    "$$ J^- = J(param-\\epsilon)\\tag{6}$$\n",
    "\n",
    "$$ \\partial param_{approx} = \\frac{J^+ - J^-}{2 \\, \\epsilon}\\tag{7}$$\n",
    "\n",
    "And then\n",
    "$$ gradientCheck = \\frac {\\| \\partial param - \\partial param_{approx} \\|_2}{\\| \\partial param \\|_2 + \\| \\partial param_{approx} \\|_2 } \\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientCheck(parameters, gradients, layers_dims, actFunctions, X, Y, epsilon = 1e-7):\n",
    "    \n",
    "    L = len(parameters) // 3\n",
    "    \n",
    "    gradsArray = GrdtDictToNpArray(gradients)\n",
    "    paramArray = ParamDictToNpArray(parameters)\n",
    "    \n",
    "    numWeightsAndBiases = gradsArray.shape[0]\n",
    "    \n",
    "    gradApprox = np.zeros((numWeightsAndBiases, 1))\n",
    "    \n",
    "    J_plus = np.zeros((numWeightsAndBiases, 1))\n",
    "    J_minus = np.zeros((numWeightsAndBiases, 1))\n",
    "    \n",
    "    # Compute gradApprox\n",
    "    for i in range(numWeightsAndBiases):\n",
    "        \n",
    "        paramPlus = np.copy(paramArray)\n",
    "        paramPlus[i][0] = paramPlus[i][0] + epsilon\n",
    "        _, _, AL = globalForwardPropagation(X, ParamNpArrayToDict(paramPlus, layers_dims, actFunctions))\n",
    "        J_plus[i] = computeCost(AL, Y)\n",
    "        \n",
    "        paramMinus = np.copy(paramArray)\n",
    "        paramMinus[i][0] = paramMinus[i][0] - epsilon     \n",
    "        _, _, AL = globalForwardPropagation(X, ParamNpArrayToDict(paramMinus, layers_dims, actFunctions))\n",
    "        J_minus[i] = computeCost(AL, Y)\n",
    "        \n",
    "        gradApprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "    \n",
    "    grdtCheck = np.linalg.norm(gradsArray - gradApprox) / (np.linalg.norm(gradsArray) + np.linalg.norm(gradApprox))\n",
    "\n",
    "    return grdtCheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Model construction with Gradient checking ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelWithGradientCheck(X, Y, layers_dims, actFunctions, learning_rate = 0.0008, num_iterations = 2, print_cost = False):\n",
    "    \n",
    "    # Initialize the weights and the bias\n",
    "    parameters = initialize(layers_dims, actFunctions)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        Zs, As, AL = globalForwardPropagation(X, parameters)\n",
    "        \n",
    "        cost = computeCost(AL, Y)\n",
    "        \n",
    "        grads = globalBackwardPropagation(Y, Zs, As, parameters)\n",
    "        \n",
    "        grdtCheck = gradientCheck(parameters, grads, layers_dims, actFunctions, X, Y)\n",
    "            \n",
    "        parameters = updateParameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        if print_cost:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "        if grdtCheck > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! gradientCheck = \" + str(grdtCheck) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"The Backward propagation works perfectly fine! gradientCheck = \" + str(grdtCheck) + \"\\033[0m\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Try with the cat training set and the 2_L-LayerNN architecture ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgLoaded, training_label, testing_imgLoaded, testing_label, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reshape and normalize the images to have a $(height*width*3 \\text{ x } 1)$ matrix with pixels values between $0$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_img = training_imgLoaded.reshape(training_imgLoaded.shape[0], -1).T / 255.\n",
    "testing_img = testing_imgLoaded.reshape(testing_imgLoaded.shape[0], -1).T / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pixels: 12288\n",
      "Number of training data: 209\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pixels: %i\" %(training_img.shape[0]))\n",
    "print(\"Number of training data: %i\" %(training_img.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we choose the dimensions of each layers and the activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerDims = [12288, 20, 7, 5, 1]\n",
    "actFunctions =  [\"ReLU\", \"ReLU\", \"ReLU\", \"sigmoid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Layers: 4\n",
      "  1 / W(20, 12288) / b(20, 1) / ReLU\n",
      "  2 / W(7, 20) / b(7, 1) / ReLU\n",
      "  3 / W(5, 7) / b(5, 1) / ReLU\n",
      "  4 / W(1, 5) / b(1, 1) / sigmoid\n",
      "\n",
      "Cost after iteration 0: 0.764218\n",
      "\u001b[92mThe Backward propagation works perfectly fine! gradientCheck = 1.679466501538512e-08\u001b[0m\n",
      "Cost after iteration 1: 0.725598\n",
      "\u001b[92mThe Backward propagation works perfectly fine! gradientCheck = 2.254014894917003e-08\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parameters = modelWithGradientCheck(training_img, training_label, layerDims, actFunctions, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
